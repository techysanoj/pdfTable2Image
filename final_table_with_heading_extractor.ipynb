{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Heading from the PDF page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from typing import Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadingType(Enum):\n",
    "    MAJOR = \"major\"\n",
    "    MINOR = \"minor\"\n",
    "    SUB = \"sub\"\n",
    "    NORMAL = \"normal\"\n",
    "\n",
    "@dataclass\n",
    "class TextMetadata:\n",
    "    text: str\n",
    "    font_name: str\n",
    "    font_size: float\n",
    "    is_bold: bool\n",
    "    is_italic: bool\n",
    "    color: tuple\n",
    "    coordinates: tuple\n",
    "    block_type: HeadingType\n",
    "\n",
    "class PDFMetadataExtractor:\n",
    "    def __init__(self, pdf_path: str, header_height_percent: float = 10, footer_height_percent: float = 10):\n",
    "        self.pdf_path = Path(pdf_path)\n",
    "        if not self.pdf_path.exists():\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "        \n",
    "        self.doc = fitz.open(pdf_path)\n",
    "        self.header_height_percent = header_height_percent\n",
    "        self.footer_height_percent = footer_height_percent\n",
    "        \n",
    "    def _get_page_regions(self, page: fitz.Page) -> Tuple[float, float, float]:\n",
    "        \"\"\"Calculate header, content, and footer regions.\"\"\"\n",
    "        page_height = page.rect.height\n",
    "        header_bottom = page_height * (self.header_height_percent / 100)\n",
    "        footer_top = page_height * (1 - self.footer_height_percent / 100)\n",
    "        return header_bottom, footer_top, page_height\n",
    "    \n",
    "    def _is_in_content_area(self, y_position: float, header_bottom: float, footer_top: float) -> bool:\n",
    "        \"\"\"Check if a position is in the main content area.\"\"\"\n",
    "        return header_bottom <= y_position <= footer_top\n",
    "\n",
    "    def _analyze_font_sizes(self, page: fitz.Page, header_bottom: float, footer_top: float) -> Dict[str, float]:\n",
    "        \"\"\"Analyze font sizes to determine dynamic thresholds for headings.\"\"\"\n",
    "        font_sizes = []\n",
    "        dict_data = page.get_text(\"dict\")\n",
    "        \n",
    "        for block in dict_data.get(\"blocks\", []):\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    y_pos = span.get(\"bbox\", (0, 0, 0, 0))[1]\n",
    "                    if self._is_in_content_area(y_pos, header_bottom, footer_top):\n",
    "                        if span.get(\"size\"):\n",
    "                            font_sizes.append(span[\"size\"])\n",
    "        \n",
    "        if not font_sizes:\n",
    "            return {\"avg\": 0, \"major\": 0, \"minor\": 0, \"sub\": 0}\n",
    "        \n",
    "        avg_size = sum(font_sizes) / len(font_sizes)\n",
    "        max_size = max(font_sizes)\n",
    "        min_size = min(font_sizes)\n",
    "        \n",
    "        return {\n",
    "            \"avg\": avg_size,\n",
    "            \"major\": max_size * 0.9,  # Dynamic threshold closer to the max\n",
    "            \"minor\": avg_size * 1.2,  # Minor headings often slightly larger than average\n",
    "            \"sub\": avg_size * 1.05   # Sub headings are slightly above the average\n",
    "        }\n",
    "\n",
    "    def _determine_heading_type(\n",
    "        self, \n",
    "        font_size: float, \n",
    "        is_bold: bool, \n",
    "        thresholds: Dict[str, float]\n",
    "    ) -> HeadingType:\n",
    "        \"\"\"Determine heading type based on font size and properties.\"\"\"\n",
    "        if font_size >= thresholds[\"major\"]:\n",
    "            return HeadingType.MAJOR\n",
    "        elif font_size >= thresholds[\"minor\"] and is_bold:\n",
    "            return HeadingType.MINOR\n",
    "        elif font_size >= thresholds[\"sub\"] and (is_bold or self._is_italic):\n",
    "            return HeadingType.SUB\n",
    "        return HeadingType.NORMAL\n",
    "    \n",
    "    def _is_bold(self, font_name: str) -> bool:\n",
    "        \"\"\"Check if the font is bold.\"\"\"\n",
    "        bold_indicators = ['bold', 'heavy', 'black', 'extrabold', 'ultrabold', 'demibold']\n",
    "        return any(indicator in font_name.lower() for indicator in bold_indicators)\n",
    "    \n",
    "    def _is_italic(self, font_name: str) -> bool:\n",
    "        \"\"\"Check if the font is italic.\"\"\"\n",
    "        italic_indicators = ['italic', 'oblique']\n",
    "        return any(indicator in font_name.lower() for indicator in italic_indicators)\n",
    "\n",
    "    def extract_page_metadata(self, page_num: int) -> List[TextMetadata]:\n",
    "        \"\"\"Extract text metadata from a specific page.\"\"\"\n",
    "        page = self.doc[page_num]\n",
    "        header_bottom, footer_top, _ = self._get_page_regions(page)\n",
    "        font_thresholds = self._analyze_font_sizes(page, header_bottom, footer_top)\n",
    "        \n",
    "        metadata_list = []\n",
    "        dict_data = page.get_text(\"dict\")\n",
    "        \n",
    "        for block in dict_data.get(\"blocks\", []):\n",
    "            last_y_pos = None\n",
    "            current_text = \"\"\n",
    "            current_metadata = None\n",
    "            \n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    bbox = span.get(\"bbox\", (0, 0, 0, 0))\n",
    "                    y_pos = bbox[1]\n",
    "                    \n",
    "                    if not self._is_in_content_area(y_pos, header_bottom, footer_top):\n",
    "                        continue\n",
    "                    \n",
    "                    text = span.get(\"text\", \"\").strip()\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    \n",
    "                    is_bold = self._is_bold(span.get(\"font\", \"\"))\n",
    "                    font_size = span.get(\"size\", 0)\n",
    "                    \n",
    "                    heading_type = self._determine_heading_type(\n",
    "                        font_size,\n",
    "                        is_bold,\n",
    "                        font_thresholds\n",
    "                    )\n",
    "                    \n",
    "                    # Check if this span should be merged with the previous one\n",
    "                    if (current_metadata and \n",
    "                        abs(y_pos - last_y_pos) < (font_size * 1.5) and \n",
    "                        heading_type == current_metadata.block_type):\n",
    "                        \n",
    "                        current_text += \" \" + text\n",
    "                    else:\n",
    "                        # Save the previous metadata if exists\n",
    "                        if current_metadata:\n",
    "                            current_metadata.text = current_text\n",
    "                            metadata_list.append(current_metadata)\n",
    "                        \n",
    "                        # Start a new text metadata\n",
    "                        current_text = text\n",
    "                        current_metadata = TextMetadata(\n",
    "                            text=current_text,\n",
    "                            font_name=span.get(\"font\", \"\"),\n",
    "                            font_size=font_size,\n",
    "                            is_bold=is_bold,\n",
    "                            is_italic=self._is_italic(span.get(\"font\", \"\")),\n",
    "                            color=span.get(\"color\", (0, 0, 0)),\n",
    "                            coordinates=bbox,\n",
    "                            block_type=heading_type\n",
    "                        )\n",
    "                    \n",
    "                    last_y_pos = y_pos\n",
    "            \n",
    "            # Save the last metadata in the block if it exists\n",
    "            if current_metadata:\n",
    "                current_metadata.text = current_text\n",
    "                metadata_list.append(current_metadata)\n",
    "        \n",
    "        return metadata_list\n",
    "\n",
    "    def extract_headings_explicit(self) -> Dict[int, Dict[str, List[str]]]:\n",
    "        \"\"\"Extract and explicitly list headings categorized by type for each page.\"\"\"\n",
    "        all_headings = {}\n",
    "\n",
    "        for page_num in range(self.doc.page_count):\n",
    "            page_metadata = self.extract_page_metadata(page_num)\n",
    "            all_headings[page_num + 1] = {  # Page numbers are 1-indexed\n",
    "                \"major_headings\": [],\n",
    "                \"minor_headings\": [],\n",
    "                \"sub_headings\": []\n",
    "            }\n",
    "\n",
    "            for meta in page_metadata:\n",
    "                if meta.block_type == HeadingType.MAJOR:\n",
    "                    all_headings[page_num + 1][\"major_headings\"].append(meta.text)\n",
    "                elif meta.block_type == HeadingType.MINOR:\n",
    "                    all_headings[page_num + 1][\"minor_headings\"].append(meta.text)\n",
    "                elif meta.block_type == HeadingType.SUB:\n",
    "                    all_headings[page_num + 1][\"sub_headings\"].append(meta.text)\n",
    "\n",
    "        return all_headings\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.doc.close()\n",
    "\n",
    "def extract_headings_from_page(\n",
    "        pdf_path: str,\n",
    "        page_number: int,\n",
    "        header_height_percent: float = 5,\n",
    "        footer_height_percent: float = 5\n",
    "    ) -> Tuple[List[str], List[str], List[str]]:\n",
    "        \"\"\"\n",
    "        Extracts major, minor, and sub-headings from a specific page of the PDF.\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): Path to the PDF file.\n",
    "            page_number (int): 1-indexed page number to extract headings from.\n",
    "            header_height_percent (float): Percentage of the page height to treat as header.\n",
    "            footer_height_percent (float): Percentage of the page height to treat as footer.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], List[str], List[str]]: Arrays of major, minor, and sub-headings.\n",
    "        \"\"\"\n",
    "        with PDFMetadataExtractor(pdf_path, header_height_percent, footer_height_percent) as extractor:\n",
    "            # Extract metadata for the specific page (convert to 0-indexed)\n",
    "            page_metadata = extractor.extract_page_metadata(page_number - 1)\n",
    "            \n",
    "            # Initialize arrays for headings\n",
    "            major_headings = []\n",
    "            minor_headings = []\n",
    "            sub_headings = []\n",
    "            \n",
    "            # Categorize the extracted metadata\n",
    "            for meta in page_metadata:\n",
    "                if meta.block_type == HeadingType.MAJOR:\n",
    "                    major_headings.append(meta.text)\n",
    "                elif meta.block_type == HeadingType.MINOR:\n",
    "                    minor_headings.append(meta.text)\n",
    "                elif meta.block_type == HeadingType.SUB:\n",
    "                    sub_headings.append(meta.text)\n",
    "            \n",
    "            return major_headings, minor_headings, sub_headings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Different Kind of Headings from the PDF Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major Headings: ['Tests In SuperSport Park, Centurion']\n",
      "Minor Headings: []\n",
      "Sub Headings: ['India Bowling']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"../Pre_Match Report New.pdf\"  # Replace with your actual PDF file path\n",
    "    page_number = 14  # Replace with the specific page number (1-indexed)\n",
    "\n",
    "    major, minor, sub = extract_headings_from_page(pdf_path, page_number)\n",
    "    print(f\"Major Headings: {major}\")\n",
    "    print(f\"Minor Headings: {minor}\")\n",
    "    print(f\"Sub Headings: {sub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Table From PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import os\n",
    "import pytesseract\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFTableExtractor:\n",
    "    def __init__(self, pdf_path, output_dir=\"extracted_tables\", heading_margin=200):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.pages = None\n",
    "        self.heading_margin = heading_margin\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def convert_pdf_to_images(self):\n",
    "        \"\"\"Convert PDF pages to images.\"\"\"\n",
    "        try:\n",
    "            self.pages = convert_from_path(self.pdf_path, dpi=300)\n",
    "            return self.pages\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting PDF to images: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def detect_text_regions(self, gray_image):\n",
    "        \"\"\"Detect text regions that might indicate table headers.\"\"\"\n",
    "        try:\n",
    "            data = pytesseract.image_to_data(gray_image, output_type=pytesseract.Output.DICT)\n",
    "            potential_headers = []\n",
    "            \n",
    "            for i in range(len(data['text'])):\n",
    "                if int(data['conf'][i]) > 60:  # Filter for confident text detection\n",
    "                    text = data['text'][i].strip().lower()\n",
    "                    if len(text) > 3:  # Filter out very short text\n",
    "                        x = data['left'][i]\n",
    "                        y = data['top'][i]\n",
    "                        w = data['width'][i]\n",
    "                        h = data['height'][i]\n",
    "                        potential_headers.append((x, y, w, h, text))\n",
    "            # printing the potential headers\n",
    "            # print(\"potential headers: \", potential_headers)\n",
    "            return potential_headers\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting text regions: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def is_likely_table(self, contour, image_shape, text_regions):\n",
    "        \"\"\"Enhanced table detection with improved filtering for headers/footers.\"\"\"\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        area = cv2.contourArea(contour)\n",
    "        rect_area = w * h\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        \n",
    "        # Basic shape metrics\n",
    "        circularity = 4 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0\n",
    "        rectangularity = area / rect_area if rect_area > 0 else 0\n",
    "        aspect_ratio = w / h if h > 0 else 0\n",
    "        relative_size = area / (image_shape[0] * image_shape[1])\n",
    "\n",
    "        # Filter out regions that are likely headers or footers\n",
    "        # Headers and footers typically span the full width of the page\n",
    "        page_width = image_shape[1]\n",
    "        is_full_width = w > page_width * 0.9\n",
    "        \n",
    "        # Headers and footers are usually at the top or bottom 10% of the page\n",
    "        page_height = image_shape[0]\n",
    "        is_at_edge = y < page_height * 0.1 or (y + h) > page_height * 0.9\n",
    "        \n",
    "        # If region is both full-width and at page edge, likely a header/footer\n",
    "        if is_full_width and is_at_edge:\n",
    "            return False\n",
    "\n",
    "        # Check for table-like structure\n",
    "        has_headers = False\n",
    "        header_text_count = 0\n",
    "        for tx, ty, tw, th, text in text_regions:\n",
    "            if (x <= tx <= x + w and y <= ty <= y + h):\n",
    "                has_headers = True\n",
    "                header_text_count += 1\n",
    "\n",
    "        # Require multiple text regions for a valid table\n",
    "        if header_text_count < 2:\n",
    "            return False\n",
    "\n",
    "        # More lenient criteria for regions with headers\n",
    "        if has_headers:\n",
    "            return (\n",
    "                circularity < 0.9 and\n",
    "                rectangularity > 0.3 and\n",
    "                0.2 < aspect_ratio < 25.0 and\n",
    "                0.01 < relative_size < 0.8 and  # Adjusted size constraints\n",
    "                w > 50 and h > 50 and  # Minimum dimensions\n",
    "                w < page_width * 0.95  # Should not span entire page width\n",
    "            )\n",
    "        else:\n",
    "            # Stricter criteria for regions without headers\n",
    "            return (\n",
    "                circularity < 0.7 and\n",
    "                rectangularity > 0.6 and\n",
    "                0.5 < aspect_ratio < 15.0 and\n",
    "                0.02 < relative_size < 0.7 and\n",
    "                w > 100 and h > 50 and\n",
    "                w < page_width * 0.9\n",
    "            )\n",
    "\n",
    "    def detect_tables(self, page_image):\n",
    "        \"\"\"Enhanced table detection with improved filtering.\"\"\"\n",
    "        try:\n",
    "            img_array = np.array(page_image)\n",
    "            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # Detect text regions first\n",
    "            text_regions = self.detect_text_regions(gray)\n",
    "            \n",
    "            # Multiple threshold approaches\n",
    "            thresh_adaptive = cv2.adaptiveThreshold(\n",
    "                gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 15, 3\n",
    "            )\n",
    "            \n",
    "            _, thresh_binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "            thresh = cv2.bitwise_or(thresh_adaptive, thresh_binary)\n",
    "            \n",
    "            # Morphological operations\n",
    "            kernel_small = np.ones((3, 3), np.uint8)\n",
    "            kernel_large = np.ones((5, 5), np.uint8)\n",
    "            \n",
    "            thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel_small)\n",
    "            thresh = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel_small)\n",
    "            thresh = cv2.dilate(thresh, kernel_large, iterations=1)\n",
    "\n",
    "            # Find contours\n",
    "            contours_ext, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contours_tree, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            all_contours = contours_ext + contours_tree\n",
    "            table_regions = set()\n",
    "            min_area = 5000  # Increased minimum area\n",
    "            \n",
    "            for contour in all_contours:\n",
    "                if cv2.contourArea(contour) > min_area and self.is_likely_table(contour, gray.shape, text_regions):\n",
    "                    x, y, w, h = cv2.boundingRect(contour)\n",
    "                    \n",
    "                    # Check for table structure\n",
    "                    roi = thresh[y:y+h, x:x+w]\n",
    "                    h_kernel_size = max(w//8, 20)\n",
    "                    v_kernel_size = max(h//8, 20)\n",
    "                    \n",
    "                    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (h_kernel_size, 1))\n",
    "                    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, v_kernel_size))\n",
    "                    \n",
    "                    horizontal_lines = cv2.morphologyEx(roi, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "                    vertical_lines = cv2.morphologyEx(roi, cv2.MORPH_OPEN, vertical_kernel)\n",
    "                    \n",
    "                    # Require either lines or multiple text regions\n",
    "                    has_lines = (np.sum(horizontal_lines) > roi.size * 0.001 or \n",
    "                               np.sum(vertical_lines) > roi.size * 0.001)\n",
    "                    \n",
    "                    text_in_region = [t for t in text_regions if \n",
    "                                    x <= t[0] <= x + w and y <= t[1] <= y + h]\n",
    "                    \n",
    "                    if has_lines or len(text_in_region) >= 3:  # Require at least 3 text regions\n",
    "                        # Don't extend margin for headers if near page edge\n",
    "                        if y > gray.shape[0] * 0.1:  # Only if not at top of page\n",
    "                            y = max(0, y - self.heading_margin)\n",
    "                            h = h + self.heading_margin\n",
    "                        table_regions.add((x, y, w, h))\n",
    "\n",
    "            return sorted(list(table_regions), key=lambda x: x[1])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting tables: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    \n",
    "    def extract_top_text_from_image(self, image):\n",
    "        \"\"\"\n",
    "        Extract text from the top portion of an image.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image object containing the table\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted text from the top portion of the image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert PIL Image to numpy array for processing\n",
    "            img_array = np.array(image)\n",
    "            \n",
    "            # Calculate the top portion (approximately 15% of the image height)\n",
    "            height = img_array.shape[0]\n",
    "            top_portion_height = int(height * 0.4)\n",
    "            \n",
    "            # Crop the top portion\n",
    "            top_portion = image.crop((0, 0, image.width, top_portion_height))\n",
    "            \n",
    "            # Enhance the image for better text recognition\n",
    "            # Convert to grayscale\n",
    "            gray_image = top_portion.convert('L')\n",
    "            \n",
    "            # Apply adaptive thresholding\n",
    "            threshold = 200\n",
    "            binary_image = gray_image.point(lambda x: 0 if x < threshold else 255, '1')\n",
    "            \n",
    "            # Increase image size for better OCR results\n",
    "            scale_factor = 2\n",
    "            enlarged_image = binary_image.resize(\n",
    "                (binary_image.width * scale_factor, binary_image.height * scale_factor),\n",
    "                Image.Resampling.LANCZOS\n",
    "            )\n",
    "            \n",
    "            # Perform OCR using pytesseract\n",
    "            custom_config = r'--oem 3 --psm 6 -c preserve_interword_spaces=1'\n",
    "            text = pytesseract.image_to_string(enlarged_image, config=custom_config)\n",
    "            \n",
    "            # Clean up the extracted text\n",
    "            cleaned_text = ' '.join(text.split())  # Remove extra whitespace\n",
    "            cleaned_text = re.sub(r'[^\\w\\s.-]', '', cleaned_text)  # Remove special characters except dots and hyphens\n",
    "            \n",
    "            return cleaned_text.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in text extraction: {str(e)}\")\n",
    "            return \"\"\n",
    "        \n",
    "    \n",
    "    def find_match_index(self, text, search_strings):\n",
    "        \"\"\"\n",
    "        Find if any of the search strings appear in the text (case insensitive)\n",
    "        and return the index of the matching string from the search_strings list.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to search in\n",
    "        search_strings (list): List of strings to search for\n",
    "        \n",
    "        Returns:\n",
    "        int: Index of the matching string, or -1 if no match found\n",
    "        \"\"\"\n",
    "        # Convert text to lowercase for case-insensitive matching\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check each search string\n",
    "        for idx, search_str in enumerate(search_strings):\n",
    "            # Convert search string to lowercase and remove spaces\n",
    "            search_str_normalized = search_str.lower().replace(\" \", \"\")\n",
    "            text_normalized = text_lower.replace(\" \", \"\")\n",
    "            \n",
    "            # Check if the normalized search string is in the normalized text\n",
    "            if search_str_normalized in text_normalized:\n",
    "                return idx\n",
    "        \n",
    "        # Return -1 if no match is found\n",
    "        return -1\n",
    "    \n",
    "        \n",
    "    def extract_team_name(self, text):\n",
    "        \"\"\"\n",
    "        Extracts the team name from a text that starts with 'Form Guide' followed by a hyphen.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): Input text that should contain 'Form Guide - TEAM'\n",
    "        \n",
    "        Returns:\n",
    "        str: Team name after the hyphen if format matches, empty string otherwise\n",
    "        \"\"\"\n",
    "        # Convert to lower case for case-insensitive check of \"Form Guide\"\n",
    "        if text.lower().startswith(\"form guide\") and '-' in text:\n",
    "            # Split by hyphen and get the last part\n",
    "            parts = text.split('-')\n",
    "            if len(parts) > 1:\n",
    "                return parts[1].strip()\n",
    "        return \"\"\n",
    "    \n",
    "    \n",
    "    def get_first_word(self, text):\n",
    "        \"\"\"\n",
    "        Extracts the first word from a given text string.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The input text string\n",
    "        \n",
    "        Returns:\n",
    "        str: The first word from the text, or empty string if text is empty\n",
    "        \"\"\"\n",
    "        # Handle empty or None input\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Split the text by whitespace and get the first word\n",
    "        words = text.strip().split()\n",
    "        \n",
    "        # Return first word if exists, empty string otherwise\n",
    "        return words[0] if words else \"\"\n",
    "\n",
    "    \n",
    "    def find_common_word_index(self, word, text_list):\n",
    "        \"\"\"\n",
    "        Find if a word appears in any of the strings in the text_list\n",
    "        and return the index of the first matching string.\n",
    "        \n",
    "        Parameters:\n",
    "        word (str): The word to search for\n",
    "        text_list (list): List of strings to search in\n",
    "        \n",
    "        Returns:\n",
    "        int: Index of the matching string, or -1 if no match found\n",
    "        \"\"\"\n",
    "        # Convert search word to lowercase\n",
    "        word = word.lower()\n",
    "        \n",
    "        # Check each string in the list\n",
    "        for idx, text in enumerate(text_list):\n",
    "            # Convert text to lowercase and split into words\n",
    "            text_words = text.lower().split()\n",
    "            \n",
    "            # Check if the word exists in the current text\n",
    "            if word in text_words:\n",
    "                return idx\n",
    "                \n",
    "        # Return -1 if no match is found\n",
    "        return -1\n",
    "    \n",
    "\n",
    "    def is_region_overlapping(self, region, processed_regions):\n",
    "        \"\"\"Check if a region overlaps with any of the processed regions.\"\"\"\n",
    "        x1, y1, w1, h1 = region\n",
    "        for px, py, pw, ph in processed_regions:\n",
    "            if not (x1 + w1 < px or px + pw < x1 or y1 + h1 < py or py + ph < y1):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def extract_text_from_image(self, image):\n",
    "        \"\"\"Extract text from an image using OCR.\"\"\"\n",
    "        try:\n",
    "            text = pytesseract.image_to_string(image)\n",
    "            #printing the text from the image\n",
    "            # print(\"text from the image: \", text.strip())\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from image: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    def extract_table_images(self):\n",
    "        \"\"\"Extract table images and save them in respective folders.\"\"\"\n",
    "        if not self.pages:\n",
    "            self.convert_pdf_to_images()\n",
    "        if not self.pages:\n",
    "            return\n",
    "        \n",
    "        for page_num, page in enumerate(self.pages):\n",
    "            try:\n",
    "                print(\"page_number : \", page_num)\n",
    "                if page_num < 2:\n",
    "                    major, minor, sub = extract_headings_from_page(pdf_path, page_num+1)\n",
    "                    teamName1 = \"\"\n",
    "                    teamName2 = \"\"\n",
    "                    for heading in major:\n",
    "                        if teamName1 == \"\":\n",
    "                            teamName1 = self.extract_team_name(heading)\n",
    "                            # print(teamName1)\n",
    "                        else:\n",
    "                            teamName2 = self.extract_team_name(heading)\n",
    "                            # print(teamName2)\n",
    "                    if page_num == 0:\n",
    "                        page_folder = os.path.join(self.output_dir, f\"page_{page_num + 1}_summary_of_{teamName1}_vs_{teamName2}\")\n",
    "                        os.makedirs(page_folder, exist_ok=True)  \n",
    "                    \n",
    "                    if page_num == 1:\n",
    "                        page_folder = os.path.join(self.output_dir, f\"page_{page_num + 1}_performance_of_venue_and_top_players_{teamName1}_vs_{teamName2}\")\n",
    "                        os.makedirs(page_folder, exist_ok=True)  \n",
    "                    # major, minor, sub = extract_headings_from_page(pdf_path, page_num+1)\n",
    "                    # page_folder = os.path.join(self.output_dir, f\"page_{page_num + 1}\")\n",
    "                    # os.makedirs(page_folder, exist_ok=True)\n",
    "                else:\n",
    "                    major, minor, sub = extract_headings_from_page(pdf_path, page_num+1)\n",
    "                    page_folder = os.path.join(self.output_dir, f\"page_{page_num+1}_{major[0]}\")\n",
    "                    os.makedirs(page_folder, exist_ok=True)  \n",
    "                \n",
    "                # page_folder = os.path.join(self.output_dir, f\"page_{page_num}\")\n",
    "                # print(\"saving the folder name perfectly\")\n",
    "                # os.makedirs(page_folder, exist_ok=True)\n",
    "\n",
    "                table_regions = self.detect_tables(page)\n",
    "                processed_regions = []\n",
    "                table_counter = 1\n",
    "                \n",
    "                \n",
    "                i = 0\n",
    "                while i < len(table_regions):\n",
    "                    table_name = \"None\"\n",
    "                    x, y, w, h = table_regions[i]\n",
    "                    if self.is_region_overlapping((x, y, w, h), processed_regions):\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                    current_table = page.crop((x, y, x + w, y + h))\n",
    "                    if not isinstance(current_table, Image.Image):\n",
    "                        raise ValueError(f\"Expected a PIL.Image.Image object, got {type(current_table)} instead.\")\n",
    "\n",
    "                    # text2 = self.extract_text_from_image(current_table)\n",
    "                    # print(\"text from the entire image: \", text2)\n",
    "                    text = self.extract_top_text_from_image(current_table)\n",
    "                    # print(\"text on the top of the image:\", text)\n",
    "\n",
    "                    if \"Last 5 Encounters\" in text:\n",
    "                        x_min, y_min = x, y\n",
    "                        x_max, y_max = x + w, y + h\n",
    "                        \n",
    "                        j = i + 1\n",
    "                        while j < len(table_regions):\n",
    "                            next_x, next_y, next_w, next_h = table_regions[j]\n",
    "                            if next_y > y and abs(next_x - x) < w * 0.5:\n",
    "                                x_min = min(x_min, next_x)\n",
    "                                x_max = max(x_max, next_x + next_w)\n",
    "                                y_max = max(y_max, next_y + next_h)\n",
    "                            j += 1\n",
    "\n",
    "                        combined_image = page.crop((x_min, y_min, x_max, y_max))\n",
    "                        combined_image.save(os.path.join(page_folder, f\"{table_counter+1}_last_5_encounters.png\"))\n",
    "                        processed_regions.append((x_min, y_min, x_max - x_min, y_max - y_min))\n",
    "                        \n",
    "                    else:\n",
    "                        if page_num < 2:\n",
    "                            # print(\"Major: \", major)\n",
    "                            # index = next((i for i, item in enumerate(major) if re.search(re.escape(item), text, re.IGNORECASE)), -1)\n",
    "                            \n",
    "                            index = self.find_match_index(text, major)\n",
    "                            if index == -1:\n",
    "                                index = next((i for i, item in enumerate(major) if item in text), -1)\n",
    "                            if index != -1:\n",
    "                                table_name = major[index]\n",
    "                                # print(\"index:\", index, \":::::\", \"table_name:\", table_name)\n",
    "                                \n",
    "                        elif page_num >=2:\n",
    "                            # print(\"Sub: \", sub)\n",
    "                            # index = next((i for i, item in enumerate(sub) if item in text), -1)\n",
    "                            first_word = self.get_first_word(text)\n",
    "                            # print(first_word)\n",
    "                            index = self.find_match_index(text, sub)\n",
    "                            # if index == -1:\n",
    "                            #     index = next((i for i, item in enumerate(sub) if item in text), -1)\n",
    "                            if index == -1:\n",
    "                                index = self.find_common_word_index(first_word, sub)\n",
    "                                # print(first_word, \":::::\", index)\n",
    "                            if(index != -1):\n",
    "                                table_name = sub[index]\n",
    "                                # print(\"index: \", index,\":::::\", \"table_name: \", table_name)\n",
    "                        current_table.save(os.path.join(page_folder, f\"table_{table_counter}_{table_name}.png\"))\n",
    "                        processed_regions.append((x, y, w, h))\n",
    "                        table_counter += 1\n",
    "                    \n",
    "                    i += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page_num + 1}: {str(e)}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path, output_dir=\"extracted_tables\", heading_margin=93):\n",
    "    \"\"\"Process the PDF and extract tables.\"\"\"\n",
    "    extractor = PDFTableExtractor(pdf_path, output_dir, heading_margin)\n",
    "    extractor.extract_table_images()\n",
    "    print(f\"Table images saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_number :  0\n",
      "Head To Head\n",
      "Head To Head\n",
      "\n",
      "Form Guide - SOUTH AFRICA\n",
      "Form Guide - SOUTH AFRICA\n",
      "SOUTH AFRICA\n",
      "Form Guide - INDIA\n",
      "Form Guide - INDIA\n",
      "INDIA\n",
      "Venue Insights\n",
      "Venue Insights\n",
      "\n",
      "Last 5 Encounters\n",
      "Last 5 Encounters\n",
      "\n",
      "page_number :  1\n",
      "Highest Successful Run Chase\n",
      "Highest Successful Run Chase\n",
      "\n",
      "Top Performance For Both Teams\n",
      "Top Performance For Both Teams\n",
      "\n",
      "page_number :  2\n",
      "page_number :  3\n",
      "page_number :  4\n",
      "page_number :  5\n",
      "page_number :  6\n",
      "page_number :  7\n",
      "page_number :  8\n",
      "page_number :  9\n",
      "page_number :  10\n",
      "page_number :  11\n",
      "page_number :  12\n",
      "page_number :  13\n",
      "page_number :  14\n",
      "page_number :  15\n",
      "page_number :  16\n",
      "page_number :  17\n",
      "page_number :  18\n",
      "page_number :  19\n",
      "Table images saved in: SA vs IND\n",
      "Extracted images saved in: SA vs IND\n"
     ]
    }
   ],
   "source": [
    "# from pdf_table_extractor_1_new import process_pdf\n",
    "\n",
    "pdf_path = \"../Pre_Match Report New.pdf\"  # Path to your PDF file\n",
    "output_dir = \"SA vs IND\"  # Directory to store extracted table images\n",
    "heading_margin = 93  # Pixels above tables to include headings\n",
    "\n",
    "# Process the PDF and extract tables\n",
    "extracted_images = process_pdf(pdf_path, output_dir, heading_margin)\n",
    "print(f\"Extracted images saved in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
